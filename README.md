# Customer Support RAG Chatbot

**LLM-powered Customer Support Agent with Advanced RAG & Agentic System**

This project implements a **task-oriented customer support chatbot** using an **advanced Retrieval-Augmented Generation (RAG) pipeline** designed to be production-oriented, observable, and extensible. The system focuses on **high answer precision, low latency, and reduced hallucination** through multi-stage retrieval, reranking, caching, and structured generation.

---

## ğŸš€ Project Overview

* Built as a **research-to-production style project** aligned with real-world customer support use cases.
* Combines **hybrid retrieval (dense + sparse)**, **cache-augmented generation (CAG)**, and **agentic routing**.
* Designed to demonstrate best practices in **RAG architecture, context engineering, evaluation, and observability**.

---

## âœ¨ Key Features

### ğŸ”¹ Cache-Augmented Generation (CAG)

* Hybrid cache combining:

  * **Exact-match cache** for repeated or identical queries
  * **Semantic cache (FAISS / Chroma)** for near-duplicate queries
* Achieves **sub-600ms latency on cache hits**.

### ğŸ”¹ Adaptive Document Chunking

* Section-based chunking for structured documents
* Semantic chunking to preserve meaning across boundaries
* Reduces retrieval noise while maintaining context integrity

### ğŸ”¹ Multi-stage Retrieval Pipeline

1. **Hybrid Search**

   * Dense embeddings (Sentence-Transformers / domain-adapted models)
   * Sparse retrieval (BM25)

2. **Candidate Selection & Reranking**

   * Top-k retrieval
   * Cross-encoder reranking for fine-grained relevance
   * Diversity filtering to reduce redundancy

3. **Context Compression**

   * Removes irrelevant or overlapping content
   * Optimizes prompt context length before generation

### ğŸ”¹ Agentic Query Routing

* LLM-based **query rewriting and intent classification**
* Routes requests into:

  * RAG-based knowledge retrieval
  * Structured data queries (SQL, internal APIs)
  * Tool-calling workflows

### ğŸ”¹ Reliable Generation

* Enforced **structured outputs using JSON schemas**
* Deterministic formatting for downstream consumption
* Reduced runtime errors and hallucinations

---

## ğŸ§  Prompt Engineering Layer

Located in `src/prompt_templates.py`, this layer enables **controlled, role-based response generation** and system self-evaluation.

### Implemented Prompts

* **STRICT_SYSTEM_PROMPT**
  Factual, concise, and high-precision responses (suitable for regulated domains).

* **FRIENDLY_SYSTEM_PROMPT**
  Empathetic and user-friendly explanations for general customer support scenarios.

* **BALANCED_SYSTEM_PROMPT**
  Combines clarity and reasoning transparency with structured outputs.

* **EVALUATION_PROMPT**
  Enables an evaluation agent to score responses on:

  * Context relevance
  * Faithfulness
  * Precision
  * Fluency

This design prepares the system for **self-assessing and event-driven agent workflows**.

---

## ğŸ“Š RAG Evaluation & Observability

* Built evaluation pipelines using **LLM-as-a-Judge** to measure:

  * Context relevance
  * Answer faithfulness
  * Recall@k
* Added logging and tracing for:

  * Retrieval stages
  * Tool-calling decisions
  * End-to-end latency

Observed results:

* ~**20% improvement in answer precision** through hybrid retrieval and reranking
* Reduced hallucination via multi-stage context filtering

---

## ğŸ§© Context Engineering (LangChain-aligned)

This project applies **LangChain Context Engineering principles** to manage information flow effectively.

| Step                 | Goal                                      | Implementation                                |
| -------------------- | ----------------------------------------- | --------------------------------------------- |
| Define Context Types | Separate static, dynamic, persistent data | System prompts, retrieved chunks, FAISS cache |
| Write Context        | Persist intermediate reasoning & state    | Logs, agent states                            |
| Select Context       | Avoid irrelevant information              | Hybrid retrieval + metadata filtering         |
| Compress Context     | Handle long documents                     | Context summarization & pruning               |
| Isolate Context      | Prevent cross-task leakage                | Scoped agent prompts                          |
| Manage Lifecycle     | Control context evolution                 | Session-level memory                          |
| Evaluate & Tune      | Improve retrieval quality                 | Evaluation prompts                            |

---

## ğŸ—‚ Project Structure

```
src/
 â”œâ”€â”€ app.py              # Streamlit demo app
 â”œâ”€â”€ cache.py            # Hybrid CAG (exact + semantic cache)
 â”œâ”€â”€ config.py           # Model and system configuration
 â”œâ”€â”€ generator.py        # LLM generation & output formatting
 â”œâ”€â”€ retriever.py        # Multi-stage retrieval logic
 â”œâ”€â”€ rag_pipeline.py     # End-to-end pipeline orchestration
 â”œâ”€â”€ prompt_templates.py# System & evaluation prompts
 â”œâ”€â”€ models/             # Pydantic schemas for LLM I/O
notebooks/               # Embedding & pipeline experiments
data/                    # Raw and processed datasets
requirements.txt
```

---

## â–¶ï¸ Quickstart

```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\\Scripts\\activate
pip install -r requirements.txt
```

Set environment variables:

```bash
export GROQ_API_KEY=your_api_key_here
```

Run demo:

```bash
python -m src.rag_pipeline
# or
streamlit run src/app.py
```

---

## ğŸ¯ Purpose

This project is designed to demonstrate:

* Production-style **RAG system design**
* Practical **agentic workflows**
* Context engineering and evaluation best practices

It aligns directly with the **LLM-powered Customer Support Agent (Advanced RAG & Agentic System)** described in the CV and serves as a strong foundation for real-world customer support applications.

---

## ğŸ¤ Contributing

* Open issues or pull requests for improvements
* Add unit tests under `src/`
* Follow clean architecture and reproducibility practices
